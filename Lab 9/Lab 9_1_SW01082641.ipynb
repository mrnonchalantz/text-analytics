{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505ecc7c-335c-4182-9dd4-fbd85cbe8a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\A S U\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\A S U\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\A S U\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for Text Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#for topic modelling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "#Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36690e92-36cb-4352-845f-b68a6cd400dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "\"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    "\"Rafael Nadal Is Out of the Australian Open\",\n",
    "\"Biden Announces Virus Measures\",\n",
    "\"Biden's Virus Plans Meet Reality\",\n",
    "\"Where Biden's Virus Plan Stands\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2397345-ddc5-4402-8493-962eafe2436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Documents:\n",
      "[['rafael', 'nadal', 'join', 'roger', 'federer', 'missing', 'open'], ['rafael', 'nadal', 'australian', 'open'], ['biden', 'announces', 'virus', 'measure'], ['biden', 'virus', 'plan', 'meet', 'reality'], ['biden', 'virus', 'plan', 'stand']]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialize a WordNet Lemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # Tokenize the text into words and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()] # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stopwords from the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatize each token\n",
    "    return tokens # Return the preprocessed tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents] # Preprocess each document in the list\n",
    "\n",
    "# Print the preprocessed documents to see the result (as in the document output)\n",
    "print(\"Preprocessed Documents:\")\n",
    "print(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e5a035-2bce-42e2-8f74-ad32c8c3d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cfc376e-5523-447f-8ed4-c1a8a7065812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus: bag-of-words representation of the documents\n",
    "#num_topics: number of topics to be extracted by the model\n",
    "#id2word=dictionary: dictionary mapping from word IDs to words\n",
    "#passes: number of passes through the corpus during training\n",
    "\n",
    "# Train an LDA model on the corpus with 2 topics using Gensim's LdaModel class\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dd80b49-d0f5-454e-9478-d6e8e1a6dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # for each document, convert to box representation\n",
    "    bow = corpus[i] # Use the already created bag-of-words for this document\n",
    "    # get list of topic probabilities for the document\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # append the dominant topic id to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0c64956-9e69-4590-8947-071c88df0316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table with Articles and Topic:\n",
      "                                             Article  Topic\n",
      "0  Rafael Nadal Joins Roger Federer in Missing U....      0\n",
      "1         Rafael Nadal Is Out of the Australian Open      0\n",
      "2                     Biden Announces Virus Measures      1\n",
      "3                   Biden's Virus Plans Meet Reality      1\n",
      "4                    Where Biden's Virus Plan Stands      1\n",
      "\n",
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"rafael\" (weight: 0.131)\n",
      "- \"open\" (weight: 0.131)\n",
      "- \"nadal\" (weight: 0.131)\n",
      "- \"join\" (weight: 0.078)\n",
      "- \"roger\" (weight: 0.078)\n",
      "- \"federer\" (weight: 0.078)\n",
      "- \"missing\" (weight: 0.078)\n",
      "- \"australian\" (weight: 0.078)\n",
      "- \"stand\" (weight: 0.027)\n",
      "- \"plan\" (weight: 0.027)\n",
      "\n",
      "Topic 1:\n",
      "- \"virus\" (weight: 0.166)\n",
      "- \"biden\" (weight: 0.166)\n",
      "- \"plan\" (weight: 0.118)\n",
      "- \"measure\" (weight: 0.071)\n",
      "- \"announces\" (weight: 0.071)\n",
      "- \"reality\" (weight: 0.071)\n",
      "- \"meet\" (weight: 0.071)\n",
      "- \"stand\" (weight: 0.071)\n",
      "- \"nadal\" (weight: 0.024)\n",
      "- \"open\" (weight: 0.024)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Create DataFrame to display results\n",
    "df = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"\\nTable with Articles and Topic:\")\n",
    "print(df)\n",
    "\n",
    "# Print the top terms for each topic\n",
    "print(\"\\nTop Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
