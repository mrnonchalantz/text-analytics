{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635a3bdc-6730-4f9b-b008-8691e0e1ce93",
   "metadata": {},
   "source": [
    "<h1>TF IDF + Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "980c7cb0-4e10-4e2d-8159-93aca3745536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec # Needed for Word2Vec section later\n",
    "import re # For punctuation removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee77d098-8465-4f99-91cd-8716004025c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    "\"I enjoy hiking and camping in the mountains\",\n",
    "\"I like to read books and watch movies\",\n",
    "\"I prefer playing video games over sports\",\n",
    "\"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0d61c8-61e5-4c43-be3f-7c65ab410d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    tokens = word_tokenize(text) # Tokenize\n",
    "    stop_words = set(stopwords.words('english')) # Get English stop words\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and word.isalpha()] # Remove stop words and non-alphabetic tokens\n",
    "    return ' '.join(cleaned_tokens) # Join tokens back into a string (required for TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b101e8b7-c399-49ff-bb2c-36d929b25633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Dataset (TF-IDF):\n",
      "- love playing football weekends\n",
      "- enjoy hiking camping mountains\n",
      "- like read books watch movies\n",
      "- prefer playing video games sports\n",
      "- love listening music going concerts\n"
     ]
    }
   ],
   "source": [
    "dataset = [preprocess_text(doc) for doc in dataset]\n",
    "print(\"Preprocessed Dataset (TF-IDF):\")\n",
    "for doc in dataset: print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdeb8d96-b0fb-4511-8ebd-e859305dfc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                               Predicted Cluster\n",
      "-----------------------------------  -------------------\n",
      "love playing football weekends                         1\n",
      "enjoy hiking camping mountains                         0\n",
      "like read books watch movies                           1\n",
      "prefer playing video games sports                      1\n",
      "love listening music going concerts                    1\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74f57046-6e22-4d49-bd91-7288a001e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      "Cluster 1:\n",
      " love\n",
      "\n",
      " playing\n",
      "\n",
      " football\n",
      "\n",
      " weekends\n",
      "\n",
      " going\n",
      "\n",
      " sports\n",
      "\n",
      " music\n",
      "\n",
      " concerts\n",
      "\n",
      " video\n",
      "\n",
      " games\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "for ind in order_centroids[i, :10]:\n",
    "    print(' %s' % terms[ind])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c6f4c22-b38a-42f1-af55-ba7fa02cb848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55066afe-d432-454c-b6aa-d7c0a99ad566",
   "metadata": {},
   "source": [
    "<h1>Word2Vec + Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e7cfc49-eb03-4a18-bd13-1eb552da3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41f4987a-6d8e-4c6a-ab1c-1fa830f70d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    "\"I enjoy hiking and camping in the mountains\",\n",
    "\"I like to read books and watch movies\",\n",
    "\"I prefer playing video games over sports\",\n",
    "\"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "572b4f7a-ce99-4b6d-b75d-6885e85d74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    tokens = word_tokenize(text) # Tokenize\n",
    "    stop_words = set(stopwords.words('english')) # Get English stop words\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words and word.isalpha()] # Remove stop words and non-alphabetic tokens\n",
    "    return ' '.join(cleaned_tokens) # Join tokens back into a string (required for TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db46b3ee-3984-4880-999a-9c7828fcf4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Dataset (Word2Vec):\n",
      "- love playing football weekends\n",
      "- enjoy hiking camping mountains\n",
      "- like read books watch movies\n",
      "- prefer playing video games sports\n",
      "- love listening music going concerts\n"
     ]
    }
   ],
   "source": [
    "dataset = [preprocess_text(doc) for doc in dataset]\n",
    "print(\"Preprocessed Dataset (Word2Vec):\")\n",
    "for doc in dataset: print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7b1f23e-3b65-4771-b8fe-10394fbec8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [doc.split() for doc in dataset]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49216ebb-78fd-4559-afdf-a231f8ac7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in word2vec_model.wv], axis=0) for doc in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f8351fa-23b8-4bbf-807f-14ac2d809180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                               Predicted Cluster\n",
      "-----------------------------------  -------------------\n",
      "love playing football weekends                         1\n",
      "enjoy hiking camping mountains                         1\n",
      "like read books watch movies                           1\n",
      "prefer playing video games sports                      0\n",
      "love listening music going concerts                    1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Artix\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "# Tabulate the document and predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3accf55b-8b30-4df3-8096-063f0e820e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
